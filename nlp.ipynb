{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcfa6e3",
   "metadata": {},
   "source": [
    "# Swahili news classification using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abc77e",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "News media plays a crucial role in shaping public opinion, informing societies, and influencing political and social discourse. In East Africa, Swahili is one of the most widely spoken languages, serving as a unifying medium for news dissemination across multiple countries, including Tanzania, Kenya, Uganda, Rwanda, Burundi, and the Democratic Republic of Congo.\n",
    "\n",
    "With the rise of digital journalism, there has been a rapid increase in Swahili news content, necessitating the need for automated classification of Swahili news articles. This project aims to leverage Natural Language Processing (NLP) and Deep Learning to develop a model that can accurately categorize Swahili news content into predefined categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b07ae1",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "The challenge in media reporting is managing and organizing large volumes of Swahili-language news content efficiently. Manual classification is time-consuming and prone to inconsistencies, making automated categorization crucial for news platforms, media houses, and content aggregators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181f1f0",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "### Automating News Classification.\n",
    "How can we efficiently categorize Swahili news articles using machine learning and deep learning?\n",
    "### Understanding Media Trends.\n",
    "What are the most common news topics in East African media?\n",
    "\n",
    "Are there any biases in media coverage based on classification trends?\n",
    "\n",
    "### Enhancing Content Accessibility.\n",
    "How can automated classification improve information retrieval for journalists, policymakers, and the general public?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6fb9b",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "### Overview\n",
    "This dataset consists of Swahili news articles categorized into different topics. The goal is to classify news articles into predefined categories using Natural Language Processing (NLP) and Deep Learning techniques.\n",
    "\n",
    "### Data Structure\n",
    "The dataset contains the following columns:\n",
    "\n",
    "id: A unique identifier for each news article.\n",
    "\n",
    "content: The text of the news article written in Swahili.\n",
    "\n",
    "category: The label representing the category of the news article (e.g., uchumi, kitaifa, michezo).\n",
    "\n",
    "### Categories in the Data\n",
    "The dataset has multiple categories representing different types of news. Some of the common categories include:\n",
    "\n",
    "Uchumi (Economy): Articles related to business, finance, and economic activities.\n",
    "\n",
    "Kitaifa (National News): General news related to Tanzania.\n",
    "\n",
    "Michezo (Sports): News about sports teams, events, and athletes.\n",
    "\n",
    "(Other categories may exist and need to be explored further.)\n",
    "\n",
    "### Data Size and Distribution\n",
    "To understand the dataset better, key aspects to analyze include:\n",
    "\n",
    "The total number of articles.\n",
    "\n",
    "The distribution of articles across different categories (class imbalance analysis).\n",
    "\n",
    "The average length of articles in terms of word count.\n",
    "\n",
    "### Data Quality Issues\n",
    "Potential issues to check before preprocessing:\n",
    "\n",
    "Missing values: Are there any missing or empty fields?\n",
    "\n",
    "Duplicates: Are there repeated news articles?\n",
    "\n",
    "Class imbalance: Are some categories significantly overrepresented compared to others?\n",
    "\n",
    "Noise in text: Presence of irrelevant characters, symbols, or stopwords that may need cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c755ee3",
   "metadata": {},
   "source": [
    "## Problem Statement  \n",
    "The goal of this project is to build a Swahili news classification model that accurately categorizes news articles into six predefined categories:\n",
    "\n",
    "- uchumi (economy)\n",
    "\n",
    "- kitaifa (national news)\n",
    "\n",
    "- michezo (sports)\n",
    "\n",
    "- kimataifa (international news)\n",
    "\n",
    "- burudani (entertainment)\n",
    "\n",
    "- afya (health)\n",
    "\n",
    "To achieve this, we will preprocess the text data to remove noise, tokenize, and normalize the text, followed by building a classification model. To ensure a clear, reproducible, and scalable approach, we will implement the preprocessing steps within an Scikit-learn Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d1b83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07871457",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fffa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff85c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7bb81a",
   "metadata": {},
   "source": [
    "### Check the first 5 and last 5 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d348224",
   "metadata": {},
   "source": [
    "This is to check for consisteny through the data. From this we can also see the columns we  are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a033f6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23263</th>\n",
       "      <td>SW24920</td>\n",
       "      <td>Alitoa pongezi hizo alipozindua rasmi hatua y...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23264</th>\n",
       "      <td>SW4038</td>\n",
       "      <td>Na NORA DAMIAN-DAR ES SALAAM  TEKLA (si jina ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23265</th>\n",
       "      <td>SW16649</td>\n",
       "      <td>Mkuu wa Mkoa wa Njombe, Dk Rehema Nchimbi wak...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23266</th>\n",
       "      <td>SW23291</td>\n",
       "      <td>MABINGWA wa Ligi Kuu Soka Tanzania Bara, Simb...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23267</th>\n",
       "      <td>SW11778</td>\n",
       "      <td>WIKI iliyopita, nilianza makala haya yanayole...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            content category\n",
       "23263  SW24920   Alitoa pongezi hizo alipozindua rasmi hatua y...   uchumi\n",
       "23264   SW4038   Na NORA DAMIAN-DAR ES SALAAM  TEKLA (si jina ...  kitaifa\n",
       "23265  SW16649   Mkuu wa Mkoa wa Njombe, Dk Rehema Nchimbi wak...   uchumi\n",
       "23266  SW23291   MABINGWA wa Ligi Kuu Soka Tanzania Bara, Simb...  michezo\n",
       "23267  SW11778   WIKI iliyopita, nilianza makala haya yanayole...  kitaifa"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec384d",
   "metadata": {},
   "source": [
    "#### Observation:  \n",
    "1. The data maintains uniformity from top to bottom.\n",
    "2. The columns are: (id, content, category).\n",
    "3. The train data has 23268 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f35a41d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUNGE limehakikishiwa kuwa hakuna changamoto ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twiga ilicheza mechi ya kirafiki na Kenya kwe...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Miaka mitano iliyopita Harry Maguire alikuwa...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bethsheba Wambura, Dar es Salaam Msanii wa Bon...</td>\n",
       "      <td>burudani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nMwekezaji wa Klabu ya Simba, Mohammed Dewji ...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0   BUNGE limehakikishiwa kuwa hakuna changamoto ...   kitaifa\n",
       "1   Twiga ilicheza mechi ya kirafiki na Kenya kwe...   michezo\n",
       "2  ['Miaka mitano iliyopita Harry Maguire alikuwa...   michezo\n",
       "3  Bethsheba Wambura, Dar es Salaam Msanii wa Bon...  burudani\n",
       "4  \\nMwekezaji wa Klabu ya Simba, Mohammed Dewji ...   michezo"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the testing dataset\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3bf7b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>Kamati hiyo ilibainisha kuwa moja ya mapunguf...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>ARODIA PETER-DODOMA HOSPITALI ya Rufaa ya Benj...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>WAKATI mazoezi ya timu ya taifa ya Tanzania (...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>\\n\\tNa Suleiman Rashid Omar-Pemba\\n \\n\\n \\n\\tW...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>BAO pekee lililofungwa na mshambuliaji wa Yan...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label\n",
       "7333   Kamati hiyo ilibainisha kuwa moja ya mapunguf...   uchumi\n",
       "7334  ARODIA PETER-DODOMA HOSPITALI ya Rufaa ya Benj...  kitaifa\n",
       "7335   WAKATI mazoezi ya timu ya taifa ya Tanzania (...  michezo\n",
       "7336  \\n\\tNa Suleiman Rashid Omar-Pemba\\n \\n\\n \\n\\tW...  kitaifa\n",
       "7337   BAO pekee lililofungwa na mshambuliaji wa Yan...  michezo"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98ef34",
   "metadata": {},
   "source": [
    "#### Observation:  \n",
    "1. The data also maintains uniformity.\n",
    "2. The columns are (text and label). This set of data does not have an id column.However the text and content columns are similar and also label and category.\n",
    "3. The test data has 7338 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e892a8",
   "metadata": {},
   "source": [
    "### Check the details of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883c5f6",
   "metadata": {},
   "source": [
    "Here we are going to check the number of non-null values and the datatypes of the data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb117d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23268 entries, 0 to 23267\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        23268 non-null  object\n",
      " 1   content   23268 non-null  object\n",
      " 2   category  23268 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 545.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9127c7",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "1. We can see we do not have null values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ce329",
   "metadata": {},
   "source": [
    "### Check the categories in the category column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfe64e",
   "metadata": {},
   "source": [
    "This being the target we check the categories we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49c384fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['uchumi', 'kitaifa', 'michezo', 'kimataifa', 'burudani', 'afya'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986916a",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The categories are:\n",
    "1. uchumi - This is the economy category of the news.\n",
    "2. kitaifa - This is the national news category.\n",
    "3. michezo - This is the sports news category.\n",
    "4. kimataifa- this is the international news category.\n",
    "5. burudani - This is the entertainment news category.\n",
    "6. afya - this is the health news category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0932b5",
   "metadata": {},
   "source": [
    "#### Check the number of values in the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effaadb",
   "metadata": {},
   "source": [
    "Here we check the number of occurences of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0420869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kitaifa      10242\n",
       "michezo       6004\n",
       "burudani      2229\n",
       "uchumi        2028\n",
       "kimataifa     1906\n",
       "afya           859\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b97d22",
   "metadata": {},
   "source": [
    "The value counts in order of most frequent to least frequent is:\n",
    "1. kitaifa   -   10242\n",
    "2. michezo   -    6004\n",
    "3. burudani  -    2229\n",
    "4. uchumi   -     2028\n",
    "5. kimataifa  -   1906\n",
    "6. afya     -      859"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba65b86",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f712c9",
   "metadata": {},
   "source": [
    "In this step we are going to prepare the data for exploratory data analysis (EDA) and preprocessing. The clleaning objectives are:\n",
    "1. Lowercasing – Convert all text to lowercase to maintain uniformity.\n",
    "\n",
    "2. Removing Special Characters & Punctuation – Strip out unnecessary symbols (e.g., !?,.) to clean the text.\n",
    "\n",
    "3. Removing Stopwords – Remove common Swahili stopwords (requires a Swahili stopwords list, which exists inside the `data` folder as a csv file).\n",
    "\n",
    "4. Tokenization – Split the text into individual words for further processing.\n",
    "\n",
    "5. Lemmatization/Stemming – Normalize words to their base form. This is done by removing suffixes and prefixes (this requires a Swahili NLP package).\n",
    "\n",
    "6. Vectorization – Convert the processed text into numerical features using TF-IDF or CountVectorizer.\n",
    "\n",
    "We are goiung to implement these steps im a pipeline to automate the process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc91f0d",
   "metadata": {},
   "source": [
    "#### Check for Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb12aaa",
   "metadata": {},
   "source": [
    "Null values affect EDA and Modeling negatively and have to be removed from data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e5ee527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8060e58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "content     0\n",
       "category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836975a1",
   "metadata": {},
   "source": [
    "Both the train and test dataset do not have null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85224b",
   "metadata": {},
   "source": [
    "#### Check for duplicated entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e536af0",
   "metadata": {},
   "source": [
    "Duplicates mess with EDA accuracy and model perfomance. We need to deal with them before these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dc3fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be85c6",
   "metadata": {},
   "source": [
    "The data does not have duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd59b7e",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c64adc",
   "metadata": {},
   "source": [
    "Identity columns contribute nothing to modeling and EDA. In this case the ID column has to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'id' column before processing\n",
    "train_df = train_df.drop(columns=[\"id\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446eafd",
   "metadata": {},
   "source": [
    "#### Loading the stopwords from the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9911f",
   "metadata": {},
   "source": [
    "Stop words are common language occuring words which contribute to grammatical correctness of a text but do not hold much meaning in the message of the text. In modelling these words are noise and have to be removed. The natural language toolkit (NLTK) doesn't have a Swahili library hence we imported the stopwords from an external source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "434bc543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StopWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tuna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ilikuwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kisha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mbaya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StopWords\n",
       "0      tuna\n",
       "1   ilikuwa\n",
       "2     kisha\n",
       "3      pili\n",
       "4     mbaya"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Swahili stopwords from CSV\n",
    "stopwords_df = pd.read_csv(\"data/Common Swahili Stop-words.csv\")\n",
    "stopwords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "947c4a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'acha',\n",
       " 'afanaleki',\n",
       " 'aidha',\n",
       " 'akiwa',\n",
       " 'ala',\n",
       " 'ali',\n",
       " 'alia',\n",
       " 'aliendelea',\n",
       " 'alikuwa',\n",
       " 'aliweza',\n",
       " 'ama',\n",
       " 'ambacho',\n",
       " 'ambako',\n",
       " 'ambalo',\n",
       " 'ambamo',\n",
       " 'ambao',\n",
       " 'ambapo',\n",
       " 'ambaye',\n",
       " 'anafanya',\n",
       " 'anafikiri',\n",
       " 'anajua',\n",
       " 'anakwenda',\n",
       " 'anatakiwa',\n",
       " 'anatokea',\n",
       " 'anaye',\n",
       " 'angali',\n",
       " 'anza',\n",
       " 'atakuwa',\n",
       " 'au',\n",
       " 'b',\n",
       " 'baada',\n",
       " 'baadaye',\n",
       " 'baadhi',\n",
       " 'barabara',\n",
       " 'basi',\n",
       " 'bila',\n",
       " 'bora',\n",
       " 'budi',\n",
       " 'c',\n",
       " 'cha',\n",
       " 'chake',\n",
       " 'chako',\n",
       " 'chini',\n",
       " 'chochote',\n",
       " 'chote',\n",
       " 'd',\n",
       " 'dhidi',\n",
       " 'duu',\n",
       " 'e',\n",
       " 'ebo',\n",
       " 'ewaa',\n",
       " 'f',\n",
       " 'fauka',\n",
       " 'g',\n",
       " 'h',\n",
       " 'hadi',\n",
       " 'haiyumkini',\n",
       " 'halafu',\n",
       " 'halikadhalika',\n",
       " 'hao',\n",
       " 'haohao',\n",
       " 'hapa',\n",
       " 'hapana',\n",
       " 'hapo',\n",
       " 'haraka',\n",
       " 'harakaharaka',\n",
       " 'hasa',\n",
       " 'hasha',\n",
       " 'hata',\n",
       " 'hii',\n",
       " 'hili',\n",
       " 'hilihili',\n",
       " 'hivi',\n",
       " 'hivyo',\n",
       " 'hivyohivyo',\n",
       " 'hiyo',\n",
       " 'hiyohiyo',\n",
       " 'hizi',\n",
       " 'hizo',\n",
       " 'huko',\n",
       " 'hukohuko',\n",
       " 'huku',\n",
       " 'hukuhuku',\n",
       " 'humu',\n",
       " 'humuhumu',\n",
       " 'huo',\n",
       " 'huohuo',\n",
       " 'hususani',\n",
       " 'huu',\n",
       " 'ila',\n",
       " 'ile',\n",
       " 'ilhali',\n",
       " 'ili',\n",
       " 'ilikuwa',\n",
       " 'ingawa',\n",
       " 'ingawaje',\n",
       " 'ipi',\n",
       " 'itakuwa',\n",
       " 'je',\n",
       " 'jinsi',\n",
       " 'juu',\n",
       " 'kabla',\n",
       " 'kadhalika',\n",
       " 'kama',\n",
       " 'karibu',\n",
       " 'kati',\n",
       " 'katika',\n",
       " 'katikati',\n",
       " 'kila',\n",
       " 'kima',\n",
       " 'kingine',\n",
       " 'kisha',\n",
       " 'ku',\n",
       " 'kubwa',\n",
       " 'kule',\n",
       " 'kuliko',\n",
       " 'kumbe',\n",
       " 'kutoka',\n",
       " 'kuwa',\n",
       " 'kuweza',\n",
       " 'kwa',\n",
       " 'kwamba',\n",
       " 'kwanza',\n",
       " 'kwao',\n",
       " 'kwenda',\n",
       " 'kwenu',\n",
       " 'kwenye',\n",
       " 'kwetu',\n",
       " 'la',\n",
       " 'labda',\n",
       " 'lakini',\n",
       " 'lao',\n",
       " 'lenye',\n",
       " 'lenyewe',\n",
       " 'lina',\n",
       " 'lingine',\n",
       " 'lipi',\n",
       " 'lo',\n",
       " 'lolote',\n",
       " 'loo',\n",
       " 'lote',\n",
       " 'ma',\n",
       " 'machache',\n",
       " 'makubwa',\n",
       " 'mali',\n",
       " 'mara',\n",
       " 'masalale',\n",
       " 'masuala',\n",
       " 'mataifa',\n",
       " 'mbali',\n",
       " 'mbaya',\n",
       " 'mdogo',\n",
       " 'mh',\n",
       " 'mimi',\n",
       " 'miongoni',\n",
       " 'mkono',\n",
       " 'mkubwa',\n",
       " 'moja',\n",
       " 'moja kwa moja',\n",
       " 'mpenzi',\n",
       " 'mtakuwa',\n",
       " 'mtu',\n",
       " 'mwa',\n",
       " 'mwenye',\n",
       " 'mwenyewe',\n",
       " 'mwingine',\n",
       " 'na',\n",
       " 'na hivyo',\n",
       " 'na kwa hivyo',\n",
       " 'na kwa sababu',\n",
       " 'na kwamba',\n",
       " 'nani',\n",
       " 'nanyi',\n",
       " 'nao',\n",
       " 'nasi',\n",
       " 'nayo',\n",
       " 'ndani',\n",
       " 'ndiyo',\n",
       " 'ndogo',\n",
       " 'ngapi',\n",
       " 'ng’o',\n",
       " 'ni',\n",
       " 'nini',\n",
       " 'nje',\n",
       " 'nyingi',\n",
       " 'nyingine',\n",
       " 'nyinyi',\n",
       " 'nyote',\n",
       " 'ohoo',\n",
       " 'oyee',\n",
       " 'pale',\n",
       " 'palepale',\n",
       " 'pamoja',\n",
       " 'pia',\n",
       " 'pili',\n",
       " 'pole',\n",
       " 'polepole',\n",
       " 'sababu',\n",
       " 'salale',\n",
       " 'sana',\n",
       " 'sasa',\n",
       " 'shauri',\n",
       " 'si',\n",
       " 'siku',\n",
       " 'sio',\n",
       " 'sisi',\n",
       " 'siye',\n",
       " 'tafadhali',\n",
       " 'tagi',\n",
       " 'takribani',\n",
       " 'tena',\n",
       " 'toba',\n",
       " 'tu',\n",
       " 'tuli',\n",
       " 'tuna',\n",
       " 'tunapaswa',\n",
       " 'tutakuwa',\n",
       " 'unafanya',\n",
       " 'unajua',\n",
       " 'vema',\n",
       " 'vile',\n",
       " 'vilevile',\n",
       " 'vingi',\n",
       " 'vivyo',\n",
       " 'vya',\n",
       " 'vyake',\n",
       " 'vyangu',\n",
       " 'vyao',\n",
       " 'vyenu',\n",
       " 'vyetu',\n",
       " 'vyote',\n",
       " 'vyovyote',\n",
       " 'wa',\n",
       " 'wakati',\n",
       " 'wake',\n",
       " 'walahi',\n",
       " 'wale',\n",
       " 'wangali',\n",
       " 'wao',\n",
       " 'wengine',\n",
       " 'wewe',\n",
       " 'wote',\n",
       " 'wowote',\n",
       " 'ya',\n",
       " 'yake',\n",
       " 'yako',\n",
       " 'yangu',\n",
       " 'yao',\n",
       " 'yapata',\n",
       " 'yatakuwa',\n",
       " 'yawezekana',\n",
       " 'yenu',\n",
       " 'yetu',\n",
       " 'yeye',\n",
       " 'yeyote',\n",
       " 'yoyote',\n",
       " 'yule',\n",
       " 'yumkini',\n",
       " 'yupi',\n",
       " 'za',\n",
       " 'zaidi',\n",
       " 'zake',\n",
       " 'zako',\n",
       " 'zangu',\n",
       " 'zao',\n",
       " 'zenu',\n",
       " 'zetu',\n",
       " 'zile',\n",
       " 'zilezile',\n",
       " 'zingine'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We convert it to a set for fast lookup\n",
    "swahili_stopwords = set(stopwords_df[\"StopWords\"].dropna())\n",
    "swahili_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392c5cb",
   "metadata": {},
   "source": [
    "### Defining the suffixes and prefixes for lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493260e9",
   "metadata": {},
   "source": [
    "As defined above, lemmatization is the removal of prefixes and suffixes from data to return them to their base form. For this we require the prefix and suffix list. Below are the lists of suffixes and prefixes gotten from multiple searches across the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a048d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "swahili_suffixes = [\n",
    "            'ni', 'to', 'ua', 'ika', 'eka', 'wa', 'ka', 'sha', 'la', 'lo', 'zo', 'e', 'ye', 'mo', 'ji', 'po'\n",
    "        ]\n",
    "swahili_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96744cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'wa',\n",
       " 'ki',\n",
       " 'vi',\n",
       " 'u',\n",
       " 'zi',\n",
       " 'ku',\n",
       " 'pa',\n",
       " 'mu',\n",
       " 'ni',\n",
       " 'tu',\n",
       " 'hu',\n",
       " 'ha',\n",
       " 'me',\n",
       " 'ta',\n",
       " 'li',\n",
       " 'si',\n",
       " 'hatu',\n",
       " 'ham',\n",
       " 'hawa',\n",
       " 'hu',\n",
       " 'ha',\n",
       " 'a',\n",
       " 'ya']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swahili_prefixes = [\n",
    "            'm', 'wa', 'ki', 'vi', 'u', 'zi', 'ku', 'pa', 'mu', 'ni', 'tu', 'hu', 'ha', 'me', 'ta', 'li',\n",
    "            'si', 'hatu', 'ham', 'hawa', 'hu', 'ha', 'a', 'ya'\n",
    "        ]\n",
    "swahili_prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41aa3e",
   "metadata": {},
   "source": [
    "## Text Preprocessing Steps (Implemented in a Pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d84c0c",
   "metadata": {},
   "source": [
    "### Cleaning Function \n",
    "Here, we will have a class called `SwahiliTextCleaner`, that inherits attributes from sklearn's BaseEstimator and TransformerMixin classes.  \n",
    "We will define the various steps it will take in cleaning the text, then later apply it to both the `train_df` and the `test_df`.  \n",
    "Our `preprocessing_pipeline` will then include both the `SwahiliTextCleaner` and the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bb2da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliTextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def lemmatize(self, text):\n",
    "        # Remove prefixes only if the remaining word is reasonable\n",
    "        for prefix in swahili_prefixes:\n",
    "            if text.startswith(prefix) and len(text) > len(prefix) + 2:  # Keep at least 3 characters after removal\n",
    "                text = text[len(prefix):]\n",
    "                break  # Stop after the first valid match\n",
    "        \n",
    "        # Remove suffixes only if the remaining word is reasonable\n",
    "        for suffix in swahili_suffixes:\n",
    "            if text.endswith(suffix) and len(text) > len(suffix) + 2:\n",
    "                text = text[:-len(suffix)]\n",
    "                break  # Stop after the first valid match\n",
    "\n",
    "        return text\n",
    "    def clean_text(self, text):\n",
    "        # Convert to string (handle any non-string entries)\n",
    "        text = str(text)\n",
    "        # Remove newline (`\\n`), tab (`\\t`), and extra spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "        # Remove brackets (handles cases like row 2 in test set)\n",
    "        text = re.sub(r\"[\\[\\]]\", \"\", text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenization using NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in swahili_stopwords]\n",
    "        # Lemmatization (combined prefix + suffix removal)\n",
    "        tokens = [self.lemmatize(word) for word in tokens]\n",
    "        # Join words back to a sentence\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # coz no fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b3041",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47774049",
   "metadata": {},
   "source": [
    "This step involves preparing the clean data for modeling.\n",
    "1. Creating pipelines for vectorizing the feature and label encoding the target.\n",
    "2. Calling the train and test, Xand y for use in model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16431aa5",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4882e",
   "metadata": {},
   "source": [
    "#### Preprocessing the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a68cb0",
   "metadata": {},
   "source": [
    "Machine learning and deep learning models do deal with texts hence the need to change the text to a format which the models would be able to handle the text. Term Frequency-Inverse Document Frequency Vectorizer converts text data into numerical feature vectors that can be used by machine learning and deep learning models. TF-IDF is useful because:\n",
    "\n",
    "1. Terms that are unique to a document are given higher weight.\n",
    "\n",
    "2. Terms that are common across all documents (like \"za\" or \"na\") are downweighted.\n",
    "\n",
    "3. It helps improve the relevance of features for text classification and clustering.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c673f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"text_cleaner\", SwahiliTextCleaner()),\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=5000))  # Convert text to numerical features\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f5e44",
   "metadata": {},
   "source": [
    "#### Preprocessing the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1314eca",
   "metadata": {},
   "source": [
    "The target being categorical, we need to encode the different classes so that the model can use this format. We use label encoding which  assigns numerical labels to the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf80af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(y)\n",
    "\n",
    "# Create label encoding pipeline for target\n",
    "encoding_pipeline = Pipeline([\n",
    "    (\"label_encoder\", FunctionTransformer(encode_labels))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8bd79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to the target on training and test sets\n",
    "train_df[\"category_encoded\"] = encoding_pipeline.fit_transform(train_df[\"category\"])\n",
    "test_df[\"label_encoded\"] = encoding_pipeline.transform(test_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ed55c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to the 'content' and 'text' column only (of both train and test sets)\n",
    "train_df['preprocessed_content'] = preprocessing_pipeline.fit_transform(train_df[\"content\"])\n",
    "test_df['preprocessed_text'] = preprocessing_pipeline.transform(test_df[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376a8bd",
   "metadata": {},
   "source": [
    "#### Calling the Target and features for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "333a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"category_encoded\"]\n",
    "y_test = test_df[\"label_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7417563",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['preprocessed_content']\n",
    "X_test = test_df['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941f883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
